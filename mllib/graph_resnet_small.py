#!/usr/bin/env python
"""
Build graph for mllib.

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging

from pathlib import Path
import tensorflow as tf
import numpy as np

import keras

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging


def conv(layer_name, input_to_this_layer, channels_prev, channels_this_layer, filter_size=3, strides=1, training=True):
    """
    Defines a single convolution block without activation.

    Parameters
    ----------
    layer_name: str
        Name of this layer
    input_to_this_layer: tensor
        Data stored in the following structure: batch size, height, width, channels
    channels_prev: int
        Number of channels in the previous layer
    channels_this_layer:int
        Number of channels in the current layer
    filter_size: int
        Convolution filer kernel size (height and width)
    strides: int
        Convolution filer stride (vertical and horizontal move)
    training: bool
        True for training, False for inferring

    Returns
    -------
    Feature map: tensor
        Feature map generated by a convolution
    """
    with tf.variable_scope(layer_name) as scope:
        # First 3x3
        weights = tf.get_variable("weights", [filter_size, filter_size, channels_prev, channels_this_layer],
                                  dtype=tf.float32,
                                  initializer=tf.contrib.layers.xavier_initializer(seed=0))
        bias = tf.get_variable("bias", [channels_this_layer], dtype=tf.float32)
        z = tf.add(tf.nn.conv2d(input_to_this_layer, weights, strides=[1, strides, strides, 1], padding='SAME'), bias)

        z = tf.layers.batch_normalization(z, training)

        return z


def conv_relu(layer_name, input_to_this_layer, channels_prev, channels_this_layer, filter_size=3, strides=1, training=True):
    """
    Defines a single convolution block.

    Parameters
    ----------
    layer_name: str
        Name of this layer
    input_to_this_layer: tensor
        Data stored in the following structure: batch size, height, width, channels
    channels_prev: int
        Number of channels in the previous layer
    channels_this_layer:int
        Number of channels in the current layer
    filter_size: int
        Convolution filer kernel size (height and width)
    strides: int
        Convolution filer stride (vertical and horizontal move)
    training: bool
        True for training, False for inferring

    Returns
    -------
    Feature map: tensor
        Feature map generated by a convolution and an activation
    """
    with tf.variable_scope(layer_name) as scope:
        # First 3x3
        weights = tf.get_variable("weights", [filter_size, filter_size, channels_prev, channels_this_layer],
                                  dtype=tf.float32,
                                  initializer=tf.contrib.layers.xavier_initializer(seed=0))
        bias = tf.get_variable("bias", [channels_this_layer], dtype=tf.float32)
        w_prev_a = tf.nn.conv2d(input_to_this_layer, weights, strides=[1, strides, strides, 1], padding='SAME')
        z = tf.add(w_prev_a, bias)

        z = tf.layers.batch_normalization(z, training)

        activation = tf.nn.relu(z, name=scope.name)

        return activation


def single_resnet_block(layer_name, input_to_this_layer, channels_prev, channels_this_layer, downsample=False, training=True):
    """
    Defines a single ResNet block.

    Parameters
    ----------
    layer_name: str
        Name of this layer
    input_to_this_layer: tensor
        Data stored in the following structure: batch size, height, width, channels
    channels_prev: int
        Number of channels in the previous layer
    channels_this_layer:int
        Number of channels in the current layer
    downsample: bool
        If yes, use strides=2 to downsample.  If no, use strides=1
    training: bool
        True for training, False for inferring

    Returns
    -------
    Feature map: tensor
        Feature map generated by a single resnet activation
    """
    downsampled_identity = None

    with tf.variable_scope(layer_name) as scope:
        activation = conv_relu(layer_name + "_1", input_to_this_layer, channels_prev, channels_this_layer, 3,
                               1)  # 3x3 filter, 1 stride

        if downsample:
            strides = 2
            # Downsample original
            downsampled_identity = conv(layer_name + "i", input_to_this_layer, channels_prev, channels_this_layer, 1,
                                        strides)
        else:
            strides = 1

        z = conv(layer_name + "_2", activation, channels_this_layer, channels_this_layer, 3, strides)

        z = tf.layers.batch_normalization(z, training)

        if downsample:
            activation = tf.nn.relu(z + downsampled_identity, name=scope.name)
        else:
            activation = tf.nn.relu(z + input_to_this_layer, name=scope.name)

        return activation


def n_resnet_blocks(block_name, input_to_this_layer, num_blocks, channels_prev, channels_this_block, downsample=False, training=True):
    """

    Defines n ResNet blocks.

    Parameters
    ----------
    block_name: str
        Name of this block
    input_to_this_layer: tensor
        Data stored in the following structure: batch size, height, width, channels
    num_blocks: int
        Number of ResNet blocks
    channels_prev: int
        Number of channels in the previous block
    channels_this_block:int
        Number of channels in the current block
    downsample: bool
        If yes, use strides=2 to downsample.  If no, use strides=1
    training: bool
        True for training, False for inferring

    Returns
    -------
    Feature map: tensor
        Feature map generated by a single resnet activation
    """

    feature_map = input_to_this_layer

    # Downsample
    if downsample:
        feature_map = single_resnet_block(block_name + "1", feature_map, channels_prev, channels_this_block,
                                          downsample=True, training=training)

        for i in range(num_blocks - 1):
            feature_map = single_resnet_block(block_name + str(i + 2), feature_map, channels_this_block, channels_this_block, training=training)

        return feature_map
    else:
        for i in range(num_blocks):
            feature_map = single_resnet_block(block_name + str(i + 1), feature_map, channels_this_block, channels_this_block, training=training)

        return feature_map


def build_resnet_small(h, w, channels, classes, training=True):
    """
    Build TensorFlow graph

    Parameters
    ----------
    h: int
        Height of the image
    w: int
        Width of the image
    channels: int
        Number of channels of the image
    classes: int
        Number of classes of the dataset

    Returns
    -------
    init_op: tensor
        Operation to initialize all variables
    objective: tensor
        Optimization objective
    cost: tensor
        Per sample cost
    x_placeholder: tensor
        Placeholder to feed x input
    y_placeholder: tensor
        Placeholder to feed y input
    training: bool
        True for training, False for inferring

    Raises
    ------
    ValueError
        If an image is larger than 256 high x 256 wide
    """
    x_placeholder = tf.placeholder(tf.float32, shape=(None, h, w, channels))
    y_placeholder = tf.placeholder(tf.float32, shape=(None, classes))

    input_to_this_layer = x_placeholder

    if h != 32 or w != 32 or channels != 3:
        raise ValueError("Height, width and channels need to be 32x32x3")

    activation = conv_relu("conv1", input_to_this_layer, 3, 16, 3, 1, training=training)  # 3x3 filter, 1 stride
    activation = n_resnet_blocks("resnet1_", activation, 2, 16, 16, training=training)  # 9 blocks, 16 channels
    activation = n_resnet_blocks("resnet2_", activation, 2, 16, 32,
                                 downsample=True, training=training)  # 9 blocks, change from 16 channels to 32 channels, feature map to 16 high x 16 wide
    activation = n_resnet_blocks("resnet3_", activation, 2, 32, 64,
                                 downsample=True, training=training)  # 9 blocks, change from 32 channels to 64 channels, feature map to 8 high x 8 wide

    #activation = tf.layers.average_pooling2d(activation)
    activation = tf.reduce_mean(activation, axis=[1, 2]) # take the mean for height and width
    flat = tf.contrib.layers.flatten(activation)
    #fc1 = tf.contrib.layers.fully_connected(flat, activation_fn=tf.nn.relu, num_outputs=2048)
    fc2 = tf.contrib.layers.fully_connected(activation, activation_fn=None, num_outputs=classes)
    y_hat_softmax = tf.nn.softmax(fc2)  # you can just use fc2 for prediction if you want to further optimize

    # Set up optimizer & cost
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc2, labels=y_placeholder))

    tf.summary.scalar('Cost', cost)

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # needed for batchnorm
    with tf.control_dependencies(update_ops): # needed for batchnorm
        objective = optimizer.minimize(cost)

    init_op = tf.global_variables_initializer()  # Set up operator to assign all init values to variables

    return init_op, objective, cost, x_placeholder, y_placeholder, y_hat_softmax
